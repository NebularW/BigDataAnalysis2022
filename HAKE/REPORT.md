# Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction

## 1. 论文abstract和introduction翻译

### 1.1 论文abstract翻译

​		知识图谱嵌入，旨在将实体和关系表示为低维向量（或矩阵、张量等），已被证明是一种预测知识图谱中缺失链接的强大技术。现有的知识图谱嵌入模型主要侧重于对关系模式的建模上，如对称/不对称、反转和组合。然而，许多现有的方法未能对语义层次进行建模，而这在现实世界的应用中是很常见的。为了应对这一挑战，我们提出了一种新的知识图谱嵌入模型--即层次感知知识图谱嵌入（HAKE）--它将实体映射到极坐标系中。HAKE的灵感来自于极坐标系中的同心圆可以自然地反映出层次结构的事实。具体来说，径向坐标旨在对处于不同层次的实体进行建模，半径较小的实体预计会处于较高的层次；角坐标旨在区分处于同一层次的实体，这些实体预计会有大致相同的半径，但角度不同。实验证明，HAKE可以有效地对知识图谱中的语义层次进行建模，并在链接预测任务的基准数据集上明显优于现有的最先进方法。

### 1.2 论文introduction翻译

​		知识图谱通常是事实三要素的集合--（头实体、关系、尾实体），它以结构化的方式表示人类知识。在过去的几年里，我们见证了知识图在许多领域的巨大成就，如自然语言处理（Zhang等人，2019）、问题回答（Huang等人，2019）和推荐系统（Wang等人，2018）。

​		尽管常用的知识图谱包含数十亿个三元组，但它们仍然存在着不完整性问题，即大量的有效三元组被遗漏，因为手动寻找所有的有效三元组是不现实的。因此，知识图谱补全，也被称为知识图谱中的链接预测，最近引起了人们的关注。链接预测的目的是在已知链接的基础上自动预测实体之间缺失的链接。这是一项具有挑战性的任务，因为我们不仅需要预测两个实体之间是否存在关系，而且还需要确定它是哪种关系。

​		受词嵌入（Mikolov等人，2013）的启发，研究人员转向知识图谱的分布式表示（又称知识图谱嵌入）来处理链接预测问题。知识图谱嵌入将实体和关系视为低维向量（或矩阵，张量），可以有效地存储和计算。此外，与词嵌入的情况一样，知识图谱嵌入可以保留实体和关系的语义和内在结构。因此，除了链接预测任务外，知识图嵌入还可以用于各种下游任务，如三重分类（Lin等人，2015）、关系推理（Guo, Sun, and Hu，2019）和搜索个性化（Nguyen等人，2019）。

​		现有知识图谱嵌入模型的成功在很大程度上依赖于它们对关系的连接模式的建模能力，如对称/不对称、倒置和组合（Sun等人，2019）。例如，TransE（Bordes等人，2013），它将关系表示为转变，可以对倒置和组合模式进行建模。DistMult（Yang等人，2015），它为头部实体、关系和尾部实体之间的三向相互作用建模，可以为对称模式建模。RotatE（Sun等人，2019）将实体表示为复杂空间中的点，将关系表示为旋转，可以对包括对称/不对称、倒置和组合等的关系模式进行建模。然而，许多现有的模型未能对知识图谱中的语义层次进行建模。

​		语义层次是知识图谱中普遍存在的属性。例如，WordNet（Miller 1995）包含三元组[arbor/cassia/palm, hypernym, tree]，其中 "tree "在层次结构中比 "arbor/cassia/palm "处于更高一级。Freebase（Bollacker等人，2008年）包含[England, /location/location/contains, Pontefract/Lancaster]这个三元组，其中 "Pontefract/Lancaster "在层次结构中比 "England "低一级。虽然存在一些考虑到层次结构的工作（Xie, Liu, 和Sun，2016; Zhang等人，2018），但他们通常需要额外的数据或过程来获得层次信息。因此，找到一种能够自动有效地对语义层次结构进行建模的方法仍然是一个挑战。

​		在本文中，我们提出了一个新的知识图谱嵌入模型--即层次感知知识图谱嵌入（HAKE）。为了对语义层次进行建模，HAKE将实体区分为两类：（a）处于不同层次的实体；（b）处于同一层次的实体。受具有层次属性的实体可以被看作是一棵树的启发，我们可以使用节点（实体）的深度来模拟层次结构的不同层次。因此，我们使用模量信息来模拟（a）类中的实体，因为系数的大小可以反映深度。在上述设置下，(b)类中的实体会有大致相同的模量，这很难区分。受同一圆上的点可以有不同的相位这一事实的启发，我们使用相位信息为(b)类中的实体建模。结合系数和相位信息，HAKE将实体映射到极坐标系中，其中径向坐标对应于模量信息，角坐标对应于相位信息。实验表明，我们提出的HAKE模型不仅可以清楚地分辨出实体的语义层次，在基准数据集上的表现也显著且持续地优于几种最先进的方法。

**注意**：在本文中，我们用小写字母 $h$、$r$ 和 $t$ 分别代表头实体、关系和尾实体。三元组（$h$；$r$；$t$）表示知识图谱中的一个事实。相应的粗体小写字母**h**、**r**和**t**表示头实体、关系和尾实体的嵌入（向量）。一个向量**h**的第i个条目表示为[**h**]$$_i$$。让k表示嵌入的维度。让  $\circ$ : $${R}^n \times {R}^n \rightarrow {R}^n$$  表示两个向量的哈德码乘积，即[a $$\circ$$ b]$$_i$$ = [a]$_i$ · [b]$_i$，并且 $$\parallel ·\parallel_1$$,  $$\parallel ·\parallel_2$$ 分别表示  $${l}_1$$ 和  ${l}_2$ 范式。



## 2. 问题描述

​		语义层次是知识图谱中普遍存在的属性。尽管一些模型考虑到层次结构，但通常需要额外的数据或过程来获得层次信息。因此，找到一种能够自动有效地对语义层次结构进行建模的方法仍然是一个挑战。

### 2.1 模型分类

知识图谱嵌入模型可以分为以下3类：

- 平移距离模型：$$h+r\approx t$$ 的思想  
- 双线性模型：基于乘积的评分函数，以匹配隐含在其向量空间表示中的实体和关系的潜在语义。如RESCAL、DistMult、ComplEx、HolE等
- 基于神经网络的模型：引入卷积神经网络ConvE和ConvKB，最近图神经网络也被引入。

HAKE模型属于平移距离模型。 更具体地说，HAKE与RotatE（Sun et al.2019）有相似之处，其中作者声称他们同时使用了模量和相位信息。 但是，RotatE和HAKE之间存在两个主要区别。 详细的区别如下。

- 目的不同。 RotatE旨在对包括对称/反对称，倒置和组合的关系模式进行建模。 HAKE旨在建模语义层次结构，同时还可以建模上述所有关系模式。

- 使用模量信息的方式不同。 RotatE将关系建模为复杂空间中的旋转，无论关系是什么，都鼓励两个链接的实体具有相同的模量。 RotatE中的不同模量来自训练的不准确性。 取而代之的是，HAKE显式地对模量信息进行建模，其明显优于RotatE。

### 2.2 层次结构建模方式

另一个相关问题是如何在知识图谱中对层级结构进行建模：

- Li et al. (2016)：将实体和类别共同嵌入语义空间，并设计用于概念分类和无数据分层分类任务的模型；
- Zhang et al. (2018)：使用聚类算法对层次关系结构建模；
- Xie, Liu, and Sun (2016)：提出了TKRL，将类型信息嵌入知识图谱嵌入。（但TKRL额外引入了实体的分层类型信息）

作者的工作：

- 考虑链接预测任务，这是知识图嵌入中比较常见的任务；
- 无需使用聚类算法就可以自动学习知识图中的语义层次；
- 除了知识图中的三元组外，不需要任何其他信息。



## 3. 输入、输出、模型算法描述

### 3.1 两类实体

为了对知识图的语义层次进行建模，知识图嵌入模型必须能够区分以下两类中的实体：

（a）层次结构中不同级别的实体。 例如，“哺乳动物”和“狗”，“奔跑”和“移动”。

（b）处于同一等级的实体。 例如，“玫瑰”和“牡丹”，“货车”和“卡车”。

### 3.2 层次感知知识图谱嵌入

![image-20221003185844692](http://img.nebular.site/md/image-20221003185844692.png)

> 模型示意图
>
> 为了区分不同部分的嵌入使用 $e_m$ （$e$ 可以是 $h$ 或者 $t$）和 $r_m$ 来表示在模量部分中的实体嵌入和关系嵌入在，并使用 $e_p$ （$e$ 可以是 $h$ 或者 $t$）和 $r_p$ 来表示在相位部分中的实体嵌入和关系嵌入。

​		为了对以上两个类别进行建模，本文提出了一种层次感知的知识图嵌入模型HAKE。 HAKE由两部分组成——模量部分和相位部分，分别旨在为两个不同类别中的实体建模。

​		

​		**模量部分**目的是为层次结构中不同级别的实体建模。受有层级属性的实体可以被看成一棵树这一事实的启发，作者使用一个节点（实体）的深度来对层级的不同级别建模。模量部分公式如下：

$$
h_m \circ r_m=t_m，其中h_m,r_m \in R^k, r_m \in R^k_+
$$
相应的距离函数为：

$$
d_{r,m}(h_m,t_m)=\parallel h_m\circ r_m-t_m\parallel_2
$$
​		模型允许实体嵌入的条目为负样本，但限制关系嵌入的条目为正样本。 这是因为实体嵌入的符号可以帮助我们预测两个实体之间是否存在关系。 例如，如果在 $h$ 和 $t_1$ 之间存在关系 $r$ ，而在 $h$ 和 $t_2$ 之间不存在关系，则$(h,r,t_1)$ 是正样本，而 $(h,r,t_2)$ 是负样本。目标是最小化$d_r(h_m,t_1,m)$ 并最大化 $d_r(h_m,t_2,m)$ ，以便在正样本和负样本之间做出明确区分，对于正样本，$[h]_i$ 和 $[t_1]_i$ 倾向于共享相同的符号，因为 $[r_m]_i > 0$ 。对于负样本，如果我们随机初始化 $[h_m]_i$ 和 $[t_2,_m]_i$  的符号，那么它们的符号可能会有所不同。这样，$d_r(h_m.t_2,m)$ 可能会比 $d_r(h_m,t_1,m)$ 大，这正是我们想要的。

​		此外，我们可以期望层次结构较高级别的实体具有较小的模量，因为这些实体更接近树的根。 如果我们仅使用模量部分来嵌入知识图，则类别（b）中的实体将具有相同的模量。 此外，假设 $r$ 是反映相同语义层次的关系，则 $[r]_i$ 将趋于为1，因为 $$h \circ r \circ r = h$$ 对所有 $h$ 成立。 因此，类别（b）中实体的嵌入趋于相同，这使得很难区分这些实体。 因此，需要一个新模块来对类别（b）中的实体进行建模。



​		**相位部分**旨在在语义层次结构的相同级别上对实体进行建模。 受同一个圆上的点（即具有相同模量）可以具有不同相位这一事实的启发，我们使用相位信息来区分类别（b）中的实体。 具体地，我们将 $h_p$ 和 $t_p$ 的每个项，即 $[h_p]_i$ 和 $[t_p]_i$ 视为一个相位，并将 $r_p$ 的每个项，即 $[r_p]_i$ 作为相位变换。相位部分公式如下：

$$
(h_p+r_p)mod\space2\pi=t_p，其中 h_p,r_p,t_p\in[0,2\pi)^k
$$
相应的距离函数为：

$$
d_{r,p}(h_p,t_p)=\parallel sin((h_p+r_p-t_p)/2)\parallel_1
$$
其中 $sin(·)$ 是将正弦函数应用于输入的每个元素的运算。注意，模型使用正弦函数来测量相位之间的距离，而不是使用 $\parallel h_p+ r_p-t_p\parallel_1$ ，因为相位具有周期性特征。



​		HAKE**将模量部分和相位部分结合在一起**，将实体映射到极坐标系中，其中径向坐标和角坐标分别对应于模量部分和相位部分。 也就是说，HAKE将实体 $h$ 映射到 $[ h_m ; h_p ]$ , 其中 $h_m$ 和 $h_p$ 分别有模量部分和相位部分以及 $[·;·]$ 表示两个向量的串联。显然，$([h_m]_i, [h_p]_i)$  是极坐标系统中的二维点。 具体来说，HAKE公式如下：

​										$\begin{cases}\ h_m\circ r_m=t_m, 其中 h_m,t_m\in R^k, r_m\in R^k_+,&\\  (h_p+r_p)mod\space 2\pi=t_p, 其中h_p,t_p,r_p\in[0,2\pi)^k\end{cases}$

HAKE的距离函数为：
$$
d_r(h,t)=d_{r,m}(h_m,t_m)+\lambda d_{r,p}(h_p,t_p) ，其中 \lambda \in R 是模型学习到的参数。
$$
相应的得分函数是：
$$
f_r(h,t)=d_r(h,t)=-d_{r,t}-\lambda d_{r,p}(h,t)
$$
当两个实体具有相同的模量时，模量部分 $d_{r,m}(h_m,t_m)=0$ 。但是，相位部分 $d_{r,p}(h_p,t_p)$ 可能非常不同。通过结合模量部分和相位部分，HAKE 可以对类别（a）和类别（b）中的实体进行建模。因此，HAKE 可以对知识图谱的语义层次进行建模。



​		在评估模型时，作者发现将**混合偏差添加**到 $d_r,_m(h,t)=0$ 可以帮助改善HAKE的性能。修改后的 $d_{r,m}(h,t)$ 由下式给出：
$$
d^,_{r,m}(h,t)=\parallel h_m \circ r_m +(h_m + t_m)\circ r_m^,-t_m\parallel_2
$$
其中 $-r_m<r^,_m<1$ 是具有与 $r_m$ 具有相同维的向量。实际上上述距离函数等于：
$$
d^,_{r,m}(h,t)=\parallel h_m ((r_m+r_m^,)/ (1-r^,_m))-t_m\parallel_2
$$
其中 / 表示按元素的除法运算。如果我们令 $r_m\leftarrow (r_m+r_m^,)/(1-r_m^,)$，则当比较不同实体对的距离时，修改后的距离函数与原始函数完全相同。为了表示方便，仍热使用 $d_{r,m}(h,t)=\parallel h_m\circ r_m-t_m\parallel _2$ 表示模量部分。

### 3.3 损失函数

为了训练模型，作者使用了带有自对抗训练的负采样损失函数（(Sun et al. 2019）

![image-20221003182232071](http://img.nebular.site/md/image-20221003182232071.png)

其中$\gamma$是一个固定差，$\sigma$是sigmod函数，($$h_i^,,r,t_j^,$$)是第i个负三元组。此外，

![image-20221003185442054](http://img.nebular.site/md/image-20221003185442054.png)

是采样负三元组的概率分布，其中 $\alpha$ 是采样温度。



## 4. 评价指标及其计算公式

​		论文采用一下指标作为模型的评价指标：MRR，HITS@1，HITS@3，HITS@10。这几个指标也是知识图谱嵌入模式中最常用的几个指标。

> 公式 $rank_i$ 的简单理解:
>
> 假设知识图谱的三元组为 $(h,r,t)$,  模型通过 $(h,r)$ 预测 $t$ , 输出分数矩阵 $out\uparrow$
>
> 1. 首先对 $out\uparrow$ 升序排列(从小到大)，得到 $out\uparrow$
> 2. 真实值 $t$ 在 $out\uparrow$中排第几位就是最终的值
>
> $rank_1$ 就是排第一位，$rank_1$ = 1
>
> $rank_2$ 就是排第二位，$rank_2$ = 2
>
> ······

### 4.1 MRR

​		MRR（Mean reciprocal rank）是一个国际上通用的对搜索算法进行评价的机制。公式如下：

![image-20221003200156020](http://img.nebular.site/md/image-20221003200156020.png)

### 4.2 HITS@n

HITS(Hyperlink – Induced Topic Search) 算法是利用HubPAuthority的搜索方法。公示如下

![image-20221003200554665](http://img.nebular.site/md/image-20221003200554665.png)



## 5. 对比方法及引用出处

### 5.1 实验数据集

`WN18RR`、`FB15k-237`和`YAGO3-10`

### 5.2 对比模型

`TransE`、`DistMult`、`ConvE`、`ComplEx`、`RotatE`

此外，为了验证相位部分的作用，作者提出一个仅使用模量部分的模型——`ModE`

### 5.4 引用出处

TransE 和 RotatE的结果分别来自 Nguyen等人在2018年的工作和Sun等人在2019年的工作，其他结果来自Dettmers在2018年的工作。

- Nguyen, D. Q.; Nguyen, T. D.; Nguyen, D. Q.; and Phung, D. 2018. A novel embedding model for knowledge base completion based on convolutional neural network. In
  NAACL.
- Sun, Z.; Deng, Z.-H.; Nie, J.-Y.; and Tang, J. 2019. Rotate: Knowledge graph embedding by relational rotation in complex space. In ICLR.
- Dettmers, T.; Pasquale, M.; Pontus, S.; and Riedel, S. 2018. Convolutional 2d knowledge graph embeddings. In AAAI



## 6. 结果

### 6.1 主要结果

#### 6.1.1 论文

![image-20221003205044230](http://img.nebular.site/md/image-20221003205044230.png)

> HAKE在所有数据集上都超过了其他模型的效果。

在WN18RR和YAGO3-10数据集中，有比较明显的层次结构，所以论文的方法提升比较多。FB15K-237的关系类型更为复杂，有很多关系没有层次结构，因此相对于另外两个数据集，它的提升小一些。这也显示了，只要知识图谱中有语义的层次结构，论文的方法就会带来一定的提升。

#### 6.1.2 实际结果

![实验结果](http://img.nebular.site/md/%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.png)

> 基于论文源码以及作者给出的参数，模型训练之后在测试集获得的结果

各项评价指标得分与论文接近，并且HAKE的各项指标也高于ModE

### 6.2 关系嵌入的分析

![image-20221003205105063](http://img.nebular.site/md/image-20221003205105063.png)

​		在这一部分中，首先证明了HAKE可以通过分析关系嵌入的模来有效地建模层次结构。然后，通过分析关系嵌入的相位，说明HAKE的相位部分可以帮助区分层次结构中同一level的实体。在图2中绘制了六种关系的模分布直方图。这些关系来自WN18RR、FB15k-237和YAGO3-10。具体地说，图2a、2c、2e和2f中的关系来自WN18RR。图2d中的关系来自FB15k-237。图2b中的关系取自YAGO3-10。我们将图2中的关系分为三组。

（A） 图2c和2d中的关系将语义层次结构的同一level的实体连接起来；
（B） 图2和图2中的关系表示尾实体在层次结构中比头实体处于更高的级别；
（C） 图2和图2中的关系表示尾实体在层次结构中比头实体处于更高的级别；

​		如模型描述部分所述，我们假设层次结构中更高level的实体具有较小的模值。实验验证了我们的期望。对于ModE和HAKE，组（A）中关系嵌入的大多数的元素都取1左右的值，这导致头部实体和尾部实体具有大致相同的模。在组（B）中，大多数关系条目的值都小于1，这导致头实体的模小于尾实体。组（C）与组（B）的情况相反。这些结果表明，我们的模型能够捕捉到知识图谱中的语义层次。此外，与ModE相比，HAKE的关系嵌入模具有更低的方差，这表明HAKE可以更清晰地对层次结构进行建模。
​       如上所述，组（A）中的关系反映了相同的语义层次，并且预期具有大约1的模值。显然，仅用模部分很难区分由这些关系连接的实体。在图3中，我们绘制了组（A）中关系的相位。结果表明，由于许多相具有π值，同一层次上的实体可以用它们的相来区分。

### 6.3 实体嵌入的分析

![image-20221003205133625](http://img.nebular.site/md/image-20221003205133625.png)

​		在这一部分中，为了进一步说明HAKE可以捕获实体之间的语义层次结构，可视化了几个实体对的嵌入。本文绘制了两个模型的实体嵌入：先前的SOTA模型RotatE和本文提出的HAKE。RotatE将每个实体视为一组复数。由于复数可以看作是二维平面上的一个点，因此我们可以在二维平面上绘制实体嵌入。至于HAKE，我们已经提到它将实体映射到极坐标系中。因此，我们也可以根据它们的极坐标在二维平面上绘制由HAKE生成的实体嵌入。为了公平比较，我们将k=500。也就是说，每个图包含500个点，实体嵌入的实际尺寸为1000。注意，我们使用对数标度来更好地显示实体嵌入之间的差异。由于所有的模量值都小于1，在应用对数运算后，图中较大的半径实际上表示较小的模量。

​		图4显示了WN18RR数据集中三个三元组的可视化结果。与尾实体相比，图4a、4b和4c中的头实体在语义层次上分别处于较低水平、相似水平和较高水平。可见，HAKE的可视化结果中存在明显的同心圆，说明HAKE能够有效地对语义层次进行建模。然而，在RotatE中，在所有三个子图中的实体嵌入是混合的，这使得在层次结构中不同级别的实体很难区分。

### 6.4 消融研究

![image-20221003204138337](http://img.nebular.site/md/image-20221003204138337.png)

​		在这一部分中，对HAKE的模量部分和相位部分以及混合偏差进行了消融实验。表4显示了三个基准数据集的结果

​		我们可以看到，混合偏差几乎可以改善HAKE在所有指标上的性能。还观察到，HAKE的模部分在所有数据集上都不是性能很好，因为它无法在层次结构的同一level上区分实体。当只使用相位部分时，HAKE退化为pRotatE模型（Sun等人，2019）。它的性能优于模数部分，因为它可以很好地在层次结构中同一level上对实体进行建模。然而，本文的HAKE模型在所有数据集上的表现明显优于模部分和相位部分，这说明了将这两部分结合起来对知识图谱中的语义层次进行建模的重要性。

